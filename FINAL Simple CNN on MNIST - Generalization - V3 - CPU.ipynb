{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN on MNIST - Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Chris/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras import backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import load_model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "from cleverhans.utils_mnist import data_mnist\n",
    "from cleverhans.attacks import (BasicIterativeMethod, CarliniWagnerL2, DeepFool, ElasticNetMethod, \n",
    "                                FastFeatureAdversaries, FastGradientMethod, LBFGS, MadryEtAl, \n",
    "                                MomentumIterativeMethod, SPSA, SaliencyMapMethod, VirtualAdversarialMethod)\n",
    "from cleverhans.utils_keras import KerasModelWrapper\n",
    "\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_saved_model = True\n",
    "run_attack = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in Getting the Data\n",
    "train_start=0\n",
    "train_end=60000\n",
    "test_start=0\n",
    "test_end=10000\n",
    "\n",
    "attack_start=0\n",
    "attack_end=100\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 500\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "gen_steps = 11\n",
    "\n",
    "num_points = 10\n",
    "attack_names = ['basic_iterative', 'fast_gradient', 'madry', 'momentum_iterative']\n",
    "\n",
    "run_ident = '18'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tensorflow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.core.K.set_learning_phase(0)\n",
    "\n",
    "# Set TF random seed to improve reproducibility\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "# Create TF session and set as Keras backend session\n",
    "sess = tf.Session()\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/t10k-labels-idx1-ubyte.gz\n",
      "X_train shape: (60000, 28, 28, 1)\n",
      "X_test shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = data_mnist(train_start=train_start,\n",
    "                                              train_end=train_end,\n",
    "                                              test_start=test_start,\n",
    "                                              test_end=test_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input TF placeholder\n",
    "x = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "# Define TF model graph\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "preds = model(x)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Adversarial Generation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenAdv(object):\n",
    "    def __init__(self, session):\n",
    "        self.session = session\n",
    "        self.model = None\n",
    "        self.data = None\n",
    "        self.labels = None\n",
    "    \n",
    "    def evaluate_model(self, model, data, labels, attack_names, num_points=10):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "        attack_strengths = np.linspace(0, 0.5, num_points)\n",
    "        \n",
    "        losses = np.zeros((len(attack_names), num_points))\n",
    "        accuracies = np.zeros((len(attack_names), num_points))\n",
    "        \n",
    "        for index_name, attack_name in enumerate(attack_names):\n",
    "            print('Running attack: {}'.format(attack_name))\n",
    "            for index_strength, attack_strength in enumerate(attack_strengths):\n",
    "                print('Using attack strength: {}'.format(attack_strength))\n",
    "                loss, accuracy = self.run_attack(attack_name, attack_strength)\n",
    "                losses[index_name, index_strength] = loss\n",
    "                accuracies[index_name, index_strength] = accuracy\n",
    "            \n",
    "        return losses, accuracies\n",
    "    \n",
    "    def run_attack(self, attack_name, attack_strength):\n",
    "        wrap = KerasModelWrapper(self.model)\n",
    "        \n",
    "        if attack_name is 'basic_iterative':\n",
    "            attack = BasicIterativeMethod(wrap, sess=self.session)\n",
    "            attack_params = {'eps': attack_strength, # Default O.3\n",
    "                             'eps_iter': 0.05,\n",
    "                             'nb_iter': 10,\n",
    "                             'y': self.labels,\n",
    "                             'ord': np.inf,\n",
    "                             'clip_min': None,\n",
    "                             'clip_max': None}\n",
    "        elif attack_name is 'carlini_wagner':\n",
    "            attack = CarliniWagnerL2(wrap, sess=self.session)\n",
    "            attack_params = {'y': self.labels,\n",
    "                             'nb_classes': None,\n",
    "                             'batch_size': 1,\n",
    "                             'confidence': attack_strength, # Default 0\n",
    "                             'learning_rate': 0.005,\n",
    "                             'binary_search_steps': 5,\n",
    "                             'max_iterations': 1000,\n",
    "                             'abort_early': True,\n",
    "                             'initial_const': 0.01,\n",
    "                             'clip_min': 0,\n",
    "                             'clip_max': 1}\n",
    "        elif attack_name is 'deep_fool':\n",
    "            attack = DeepFool(wrap, sess=self.session)\n",
    "            attack_params = {'nb_candidate': attack_strength, # Default 10, INT\n",
    "                             'overshoot': 0.02,\n",
    "                             'max_iter': 50,\n",
    "                             'nb_classes': None,\n",
    "                             'clip_min': 0.0,\n",
    "                             'clip_max': 1.0}\n",
    "        elif attack_name is 'elastic_net':\n",
    "            attack = ElasticNetMethod(wrap, sess=self.session)\n",
    "            attack_params = {'y': self.labels,\n",
    "                             'nb_classes': None,\n",
    "                             'fista': True,\n",
    "                             'beta': 0.001,\n",
    "                             'decision_rule': 'EN',\n",
    "                             'batch_size': 1,\n",
    "                             'confidence': attack_strength, # Default 0\n",
    "                             'learning_rate': 0.01,\n",
    "                             'binary_search_steps': 9,\n",
    "                             'max_iterations': 1000,\n",
    "                             'abort_early': False,\n",
    "                             'initial_const': 0.001,\n",
    "                             'clip_min': 0,\n",
    "                             'clip_max': 1}\n",
    "        elif attack_name is 'fast_feature':\n",
    "            attack = FastFeatureAdversaries(wrap, sess=self.session)\n",
    "            attack_params = {'eps': attack_strength, # Default 0.3\n",
    "                             'eps_iter': 0.05,\n",
    "                             'nb_iter': 10,\n",
    "                             'ord': np.inf,\n",
    "                             'clip_min': None,\n",
    "                             'clip_max': None}\n",
    "        elif attack_name is 'fast_gradient':\n",
    "            attack = FastGradientMethod(wrap, sess=self.session)\n",
    "            attack_params = {'eps': attack_strength, # Default 0.3\n",
    "                             'ord': np.inf,\n",
    "                             'y': self.labels,\n",
    "                             'clip_min': None,\n",
    "                             'clip_max': None}\n",
    "        elif attack_name is 'lbfgs':\n",
    "            attack = LBFGS(wrap, sess=self.session)\n",
    "            attack_params = {'batch_size': 1,\n",
    "                             'binary_search_steps': 5,\n",
    "                             'max_iterations': 1000,\n",
    "                             'initial_const': attack_strength, # Default 0.01\n",
    "                             'clip_min': 0,\n",
    "                             'clip_max': 1}\n",
    "        elif attack_name is 'madry':\n",
    "            attack = MadryEtAl(wrap, sess=self.session)\n",
    "            attack_params = {'eps': attack_strength, # Default 0.3\n",
    "                             'eps_iter': 0.01,\n",
    "                             'nb_iter': 40,\n",
    "                             'y': self.labels,\n",
    "                             'ord': np.inf,\n",
    "                             'clip_min': None,\n",
    "                             'clip_max': None,\n",
    "                             'rand_init': True}\n",
    "        elif attack_name is 'momentum_iterative':\n",
    "            attack = MomentumIterativeMethod(wrap, sess=self.session)\n",
    "            attack_params = {'eps': attack_strength, # Default 0.3\n",
    "                             'eps_iter': 0.06,\n",
    "                             'nb_iter': 10,\n",
    "                             'y': self.labels,\n",
    "                             'ord': np.inf,\n",
    "                             'decay_factor': 1.0,\n",
    "                             'clip_min': None,\n",
    "                             'clip_max': None}\n",
    "        elif attack_name is 'spsa':\n",
    "            attack = SPSA(wrap, sess=self.session)\n",
    "            attack_params = {'y': self.labels,\n",
    "                             'epsilon': attack_strength, # Default None\n",
    "                             'num_steps': None,\n",
    "                             'is_targeted': False,\n",
    "                             'early_stop_loss_threshold': None,\n",
    "                             'learning_rate': 0.01,\n",
    "                             'delta': 0.01,\n",
    "                             'batch_size': 128,\n",
    "                             'spsa_iters': 1,\n",
    "                             'is_debug': False}\n",
    "        elif attack_name is 'saliency_map':\n",
    "            attack = SaliencyMapMethod(wrap, sess=self.session)\n",
    "            attack_params = {'theta': attack_strength, # Default 1.0\n",
    "                             'gamma': 1.0,\n",
    "                             'nb_classes': None,\n",
    "                             'clip_min': 0.0,\n",
    "                             'clip_max': 1.0,\n",
    "                             'symbolic_impl': True}\n",
    "        elif attack_name is 'virtual_adversarial':\n",
    "            attack = VirtualAdversarialMethod(wrap, sess=self.session)\n",
    "            attack_params = {'eps': attack_strength, # Default 2.0\n",
    "                             'num_iterations': 1,\n",
    "                             'xi': 1e-06,\n",
    "                             'clip_min': None,\n",
    "                             'clip_max': None}\n",
    "        else:\n",
    "            raise ValueError('Invalid Attack Name!')\n",
    "        \n",
    "        adv_x = attack.generate(x, **attack_params)\n",
    "        data_adv = adv_x.eval(feed_dict={x:self.data}, session=self.session)\n",
    "        \n",
    "        score = model.evaluate(data_adv, self.labels, verbose=0)\n",
    "        loss = score[0]\n",
    "        accuracy = score[1]\n",
    "        \n",
    "        return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Epoch on Security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen: 10\n",
      "Proability of Randomization: 1.0\n",
      "Rand mean (only shuffled): 0.10108333333333333\n",
      "Rand mean (Should be 1): 1.0\n",
      "Rand mean (all): 0.10108333333333333\n",
      "Running attack: basic_iterative\n",
      "Using attack strength: 0.0\n",
      "WARNING:tensorflow:From /home/Chris/tensorflow/src/cleverhans/cleverhans/attacks.py:382: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/Chris/tensorflow/src/cleverhans/cleverhans/attacks_tf.py:55: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/Chris/tensorflow/src/cleverhans/cleverhans/utils_tf.py:37: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Using attack strength: 0.05555555555555555\n",
      "Using attack strength: 0.1111111111111111\n",
      "Using attack strength: 0.16666666666666666\n",
      "Using attack strength: 0.2222222222222222\n",
      "Using attack strength: 0.2777777777777778\n",
      "Using attack strength: 0.3333333333333333\n",
      "Using attack strength: 0.38888888888888884\n",
      "Using attack strength: 0.4444444444444444\n",
      "Using attack strength: 0.5\n",
      "Running attack: fast_gradient\n",
      "Using attack strength: 0.0\n",
      "Using attack strength: 0.05555555555555555\n",
      "Using attack strength: 0.1111111111111111\n",
      "Using attack strength: 0.16666666666666666\n",
      "Using attack strength: 0.2222222222222222\n",
      "Using attack strength: 0.2777777777777778\n",
      "Using attack strength: 0.3333333333333333\n",
      "Using attack strength: 0.38888888888888884\n",
      "Using attack strength: 0.4444444444444444\n",
      "Using attack strength: 0.5\n",
      "Running attack: madry\n",
      "Using attack strength: 0.0\n",
      "Using attack strength: 0.05555555555555555\n",
      "Using attack strength: 0.1111111111111111\n",
      "Using attack strength: 0.16666666666666666\n",
      "Using attack strength: 0.2222222222222222\n",
      "Using attack strength: 0.2777777777777778\n",
      "Using attack strength: 0.3333333333333333\n",
      "Using attack strength: 0.38888888888888884\n",
      "Using attack strength: 0.4444444444444444\n",
      "Using attack strength: 0.5\n",
      "Running attack: momentum_iterative\n",
      "Using attack strength: 0.0\n",
      "WARNING:tensorflow:From /home/Chris/tensorflow/src/cleverhans/cleverhans/attacks.py:540: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Using attack strength: 0.05555555555555555\n",
      "Using attack strength: 0.1111111111111111\n",
      "Using attack strength: 0.16666666666666666\n",
      "Using attack strength: 0.2222222222222222\n",
      "Using attack strength: 0.2777777777777778\n",
      "Using attack strength: 0.3333333333333333\n",
      "Using attack strength: 0.38888888888888884\n",
      "Using attack strength: 0.4444444444444444\n",
      "Using attack strength: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Fast Gradient Sign Method (FGSM) attack object and graph\n",
    "\n",
    "gen_adv = GenAdv(sess)\n",
    "\n",
    "losses = np.zeros((gen_steps, len(attack_names), num_points))\n",
    "accuracies = np.zeros((gen_steps, len(attack_names), num_points))\n",
    "\n",
    "directory = 'model_r{}'.format(run_ident)\n",
    "if not from_saved_model and not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "    \n",
    "directory = 'results_r{}'.format(run_ident)\n",
    "if run_attack and not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "for gen in range(10, 11):\n",
    "    print('Gen: {}'.format(gen))\n",
    "    \n",
    "    #callbacks = [TensorBoard(log_dir='./logs/overfitting_r{}_g{}'.format(run_ident, gen)),\n",
    "    #           EarlyStopping(monitor='loss', min_delta=1e-5, patience=5, verbose=0, mode='auto')] # 0 - 6\n",
    "    #callbacks = [TensorBoard(log_dir='./logs/overfitting_r{}_g{}'.format(run_ident, gen)),\n",
    "    #           EarlyStopping(monitor='loss', min_delta=1e-5, patience=30, verbose=0, mode='auto')] # 6 - 9\n",
    "    callbacks = [TensorBoard(log_dir='./logs/overfitting_r{}_g{}'.format(run_ident, gen)),\n",
    "                EarlyStopping(monitor='loss', min_delta=1e-5, patience=60, verbose=0, mode='auto')] # 10\n",
    "    \n",
    "    # Shuffle Labels\n",
    "    prob_sel = gen / (gen_steps-1)\n",
    "    print('Proability of Randomization: {}'.format(prob_sel))\n",
    "    \n",
    "    # Extract Sample to Edit\n",
    "    Y_rand = np.copy(Y_train)\n",
    "    sel = np.random.choice([False, True], size=(Y_rand.shape[0],), p=(1-prob_sel, prob_sel))\n",
    "    Y_sel = Y_rand[sel,:]\n",
    "    \n",
    "    np.random.shuffle(Y_sel)\n",
    "    \n",
    "    # Debug\n",
    "    rand_equal = np.equal(Y_rand[sel,:], Y_sel)\n",
    "    rand_mean = np.mean(np.all(rand_equal, axis=1))\n",
    "    print('Rand mean (only shuffled): {}'.format(rand_mean))\n",
    "    # Debug\n",
    "    \n",
    "    Y_rand[sel,:] = np.copy(Y_sel)\n",
    "    \n",
    "    # Debug\n",
    "    rand_equal = np.equal(Y_rand[sel,:], Y_sel)\n",
    "    rand_mean = np.mean(np.all(rand_equal, axis=1))\n",
    "    print('Rand mean (Should be 1): {}'.format(rand_mean))\n",
    "    # Debug\n",
    "    \n",
    "    # Debug\n",
    "    rand_equal = np.equal(Y_rand, Y_train)\n",
    "    rand_mean = np.mean(np.all(rand_equal, axis=1))\n",
    "    print('Rand mean (all): {}'.format(rand_mean))\n",
    "    # Debug\n",
    "    \n",
    "    if from_saved_model:\n",
    "        model = load_model('model_r{}/model_g{}.h5'.format(run_ident, gen))\n",
    "    else:\n",
    "        reset_weights(model)\n",
    "        model.fit(X_train, Y_rand,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  verbose=1,\n",
    "                  validation_data=(X_test, Y_test),\n",
    "                  callbacks=callbacks)\n",
    "        model.save('model_r{}/model_g{}.h5'.format(run_ident, gen))\n",
    "\n",
    "    if run_attack:\n",
    "        loss, accuracy = gen_adv.evaluate_model(model, X_test[attack_start:attack_end, :], \n",
    "                                                Y_test[attack_start:attack_end], \n",
    "                                                attack_names, num_points=num_points)\n",
    "        losses[gen,:,:] = loss\n",
    "        accuracies[gen,:,:] = accuracy\n",
    "        \n",
    "        np.save('results_r{}/loss_g{}.npy'.format(run_ident, gen), loss)\n",
    "        np.save('results_r{}/accuracy_g{}.npy'.format(run_ident, gen), accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_attack:\n",
    "    np.save('loss_r{}.npy'.format(run_ident), losses)\n",
    "    np.save('accuracy_r{}.npy'.format(run_ident), accuracies)\n",
    "    max_loss = losses.flatten().max()\n",
    "    max_accuracy = accuracies.flatten().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_func(gen):\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    plt.subplot(121)\n",
    "    for index, attack_name in enumerate(attack_names):\n",
    "        x_plt = np.linspace(0, 0.5, num_points)\n",
    "        y_plt = losses[gen, index, :].flatten()\n",
    "        plt.plot(x_plt, y_plt, label=attack_name)\n",
    "    plt.title('Adversarial Loss')\n",
    "    plt.xlabel('Attack Strength')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(ymax=max_loss)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    for index, attack_name in enumerate(attack_names):\n",
    "        x_plt = np.linspace(0, 0.5, num_points)\n",
    "        y_plt = accuracies[gen, index, :].flatten()\n",
    "        plt.plot(x_plt, y_plt, label=attack_name)\n",
    "    plt.title('Adversarial Accuracy')\n",
    "    plt.xlabel('Attack Strength')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(ymax=max_accuracy)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573d1cdac38e4aeda77f4e0be73cbd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='gen', max=10), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if run_attack:\n",
    "    interact(plot_func, gen=widgets.IntSlider(min=0,max=gen_steps-1,step=1,value=0));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
